import os
import random
import re
import sys
import math

DAMPING = 0.85
SAMPLES = 10000


def main():
    if len(sys.argv) != 2:
        sys.exit("Usage: python pagerank.py corpus")
    corpus = crawl(sys.argv[1])
    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)
    print(f"PageRank Results from Sampling (n = {SAMPLES})")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")
    ranks = iterate_pagerank(corpus, DAMPING)
    print(f"PageRank Results from Iteration")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")


def crawl(directory):
    """
    Parse a directory of HTML pages and check for links to other pages.
    Return a dictionary where each key is a page, and values are
    a list of all other pages in the corpus that are linked to by the page.
    """
    pages = dict()

    # Extract all links from HTML files
    for filename in os.listdir(directory):
        if not filename.endswith(".html"):
            continue
        with open(os.path.join(directory, filename)) as f:
            contents = f.read()
            links = re.findall(r"<a\s+(?:[^>]*?)href=\"([^\"]*)\"", contents)
            pages[filename] = set(links) - {filename}

    # Only include links to other pages in the corpus
    for filename in pages:
        pages[filename] = set(
            link for link in pages[filename]
            if link in pages
        )

    return pages


def transition_model(corpus, page, damping_factor):
    """
    Return a probability distribution over which page to visit next,
    given a current page.

    With probability `damping_factor`, choose a link at random
    linked to by `page`. With probability `1 - damping_factor`, choose
    a link at random chosen from all pages in the corpus.
    """
    # initilize and empty dictonery of the deistrbution probability for each page linnked in the [surfer current page]
    dist_prob_dic = dict()

    # total pages
    total_pages = len(corpus)

    # setting initial probability for all pages .(1 - damping_factor) / total_pages.
    initial_Prob = (1 - damping_factor) / total_pages

    # setting initial_Prob to each page in the dist_prob_dic
    dist_prob_dic = {page: initial_Prob for page in corpus}

    # if the page does not have links..thus probability for surfer to jumb into a random page in the corpus is (1 / total_pages)
    if len(corpus[page]) == 0:
        dist_prob_dic = {page: (1 / total_pages) for page in corpus}
        return dist_prob_dic

    # else .. adding the (damping_factor / total links in the page) value to every link in the current page by the surfer
    for linked_page in corpus[page]:
        dist_prob_dic[linked_page] += (damping_factor / len(corpus[page]))

    return dist_prob_dic


def sample_pagerank(corpus, damping_factor, n):
    """
    Return PageRank values for each page by sampling `n` pages
    according to transition model, starting with a page at random.

    Return a dictionary where keys are page names, and values are
    their estimated PageRank value (a value between 0 and 1). All
    PageRank values should sum to 1.
    """
    # The first sample should be generated by choosing from a page at random.
    page = random.choice(list(corpus.keys()))

    # using the transation model to choose randomly the next page
    dist_Prob_dic = transition_model(corpus, page, damping_factor)

    # initilize the pages_visited dictonery to keep track of the pages visited
    pages_visited = dict()

    # making this sample process (n) times
    for i in range(n):
        # choose randomly from the returned [distributed dictonery of the probabilities of the random page we choose]
        pages = list(dist_Prob_dic.keys())
        probabilities = list(dist_Prob_dic.values())
        next_page = random.choices(pages, probabilities)[0]

        # using the transation model on the page that has been selected
        dist_Prob_dic = transition_model(corpus, next_page, damping_factor)

        # keep track of the page visited

        # If the page has been visited before, increment its count
        if next_page in pages_visited:
            pages_visited[next_page] += 1
        # otherwise, adding the page to the dictionery as a key with inital  value = 1
        else:
            pages_visited[next_page] = 1

    # initilizing a sample_prob dictionery to put the final result of sampling
    sample_prob = dict()

    # divide each value of the page key by n to get the sample

    # ex: if we have dice and we ant to sample 600 times .. so we count each no.1 appears how many times --
    # -- say 70 and then we divide 70 / 600 this is P(1)
    sample_prob = {page: value / n for page, value in pages_visited.items()}

    return sample_prob


def iterate_pagerank(corpus, damping_factor):

    initiL_proobability = 1 / len(corpus)
    pages_prob = {}
    old_pages_prob = {}

    # give each page in corpus the initial probability
    pages_prob = {page: initiL_proobability for page in corpus}

    # old pages probabilities
    old_pages_prob = {page: initiL_proobability for page in corpus}

    # calculating a new set of PageRank values for each page

    # calculating the secound formulla
    pr_page = 0
    iteration = True
    convergence_threshold = 0.001

    while iteration:
        # make the (old_pages_prob) holding the vlaue of (pages_prob) to hold the prev. value of (pages_prob before the new iteration
        old_pages_prob = pages_prob.copy()
        # make this iteration for every page in the corpus
        for page in corpus:
            iteration_sum = 0

            # for each page in the corpus: [page that has links inside it || page that does not has any links inside]
            for each_page in corpus:
                # if the page does not have links..thus we can assume it have links to all pages including it self
                if len(corpus[each_page]) == 0:
                    iteration_sum += old_pages_prob[each_page] / len(corpus)
                elif page in corpus[each_page]:
                    iteration_sum += old_pages_prob[each_page] / len(corpus[each_page])

                # adding the first formulla to the secound one
                pr_page = ((1 - damping_factor) / len(corpus)) + (damping_factor * iteration_sum)
                pages_prob[page] = pr_page

        # Check if all pages in this iteration have changed by less than convergence_threshold (0.001) compared to the previous rank
        exit_loop = True
        for page in pages_prob:
            difference = abs(pages_prob[page] - old_pages_prob[page])
            if difference > convergence_threshold:
                exit_loop = False
                # Break out of the loop if any page has a difference greater than convergence_threshold (0.001)
                break
        if exit_loop:
            # Exit the main iteration loop if all pages have converged(all pages has changed by less than or equal 0.001)
            iteration = False

    return pages_prob


if __name__ == "__main__":
    main()
